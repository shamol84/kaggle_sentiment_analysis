{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(train[\"Sentiment\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bd8a2ba9cba16e882adc87c3825d5b5e5cab49c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.merge(test, train[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5a1d69573b2a88682de5875559d2c9beef6ca8a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_1 = CountVectorizer()\n",
    "cv_1.fit(train[\"Phrase\"])\n",
    "\n",
    "cv_2 = CountVectorizer()\n",
    "cv_2.fit(test[\"Phrase\"])\n",
    "\n",
    "all_words = set(cv_1.vocabulary_.keys()).union(set(cv_2.vocabulary_.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b8b5dddebdebaa7b6b0a0df7bc527106db6935e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n",
    "    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n",
    "    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "train = transform(train)\n",
    "test = transform(test)\n",
    "\n",
    "dense_features = [\"phrase_count\", \"word_count\", \"Phrase\"] \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "485709e3955e7bec78ef443de133b630e5637b78",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 3\n",
    "\n",
    "train[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\n",
    "\n",
    "EMBEDDING_FILE = \"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "def get_embedding():\n",
    "    all_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    for line in f:\n",
    "        line_values = line.split()\n",
    "        word = line_values[0]\n",
    "        if len(line_values) == EMBEDDING_DIM + 1 and word in all_words:\n",
    "            coef_values = np.asarray(line_values[1:], dtype=\"float32\")\n",
    "            all_index[word] = coef_values\n",
    "    f.close()\n",
    "    return all_index\n",
    "\n",
    "embeddings_index = get_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d501bf1460d97c7853c461b9046a361863315ec",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "tokenizer = Tokenizer(filters = \"\")\n",
    "tokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_mat = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_mat[i] = embedding_vector\n",
    "        \n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen = MAX_SEQUENCE_LENGTH)\n",
    "test_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a41bc21af8b2b0551ba83f20cc2d6c4c4398c180",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights = [embedding_mat],\n",
    "                                input_length = MAX_SEQUENCE_LENGTH,\n",
    "                                trainable = True)\n",
    "    \n",
    "    dropout = SpatialDropout1D(0.3)\n",
    "    mask_layer = Masking()\n",
    "    lstm_layer = LSTM(80)\n",
    "    \n",
    "    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    dense_input = Input(shape=(len(dense_features),))\n",
    "    \n",
    "    dense_vector = BatchNormalization()(dense_input)\n",
    "    \n",
    "    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n",
    "    \n",
    "    feature_vector = concatenate([phrase_vector, dense_vector]) ## joining features\n",
    "    feature_vector_2 = Dense(80, activation=\"relu\")(feature_vector)\n",
    "    feature_vector_3 = Dense(40, activation=\"relu\")(feature_vector_2)\n",
    "    #dropout = SpatialDropout1D(0.3)\n",
    "    #feature_vector_4 = Dense(20, activation=\"relu\")(feature_vector_3)\n",
    "    \n",
    "    output = Dense(5, activation = \"softmax\")(feature_vector_3)\n",
    "    \n",
    "    model = Model(inputs = [seq_input, dense_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'nb_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-71bf1bc0f6e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-329c69ed3181>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     embedding_layer = Embedding(nb_words,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                 \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_mat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'nb_words' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "765aefdbd50695e65b76a7d259444ff86a366dfa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = np.zeros((test.shape[0], 5))\n",
    "\n",
    "for i in range(NUM_FOLDS):\n",
    "    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n",
    "    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n",
    "    y_train = enc.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n",
    "    y_val = enc.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n",
    "    \n",
    "    model = build_model()\n",
    "    model.compile(loss = \"categorical_crossentropy\", \n",
    "                  optimizer = \"nadam\", \n",
    "                  metrics = [\"acc\"])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit([train_seq, train_dense], \n",
    "              y_train, validation_data=([val_seq, val_dense], y_val),\n",
    "              epochs= 10, batch_size= 512, ###change epoch value, use early stopping criteria\n",
    "              shuffle = True, callbacks=[early_stopping], \n",
    "              verbose = 1)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n",
    "    print()\n",
    "    \n",
    "test_preds /= NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c12921cd0632e21d915b56c642064a786fde4a8e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[\"pred\"] = test_preds.argmax(axis=1)\n",
    "\n",
    "test.loc[test[\"Sentiment\"].isnull(), \"Sentiment\"] = test.loc[test[\"Sentiment\"].isnull(), \"pred\"]\n",
    "\n",
    "test[\"Sentiment\"] = test[\"Sentiment\"].astype(int)\n",
    "test[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb88a96937055288f75b254787684041c97053ac",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
